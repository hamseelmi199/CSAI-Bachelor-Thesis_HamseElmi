{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob  # For file path expansion\n",
    "import os  # For interacting with the operating system\n",
    "import numpy as np  # For numerical computing\n",
    "import pandas as pd # For data manipulation and analysis\n",
    "import matplotlib as plt  # For plotting\n",
    "import mne  # For processing EEG data\n",
    "from sklearn.linear_model import LogisticRegression  # For logistic regression classification\n",
    "from sklearn.pipeline import Pipeline  # For constructing processing pipelines\n",
    "from sklearn.preprocessing import StandardScaler  # For feature standardization\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV  # For cross-validation and hyperparameter tuning\n",
    "from sklearn.naive_bayes import GaussianNB  # For Gaussian Naive Bayes classification\n",
    "from sklearn.ensemble import RandomForestClassifier  # For random forest classification\n",
    "from sklearn.svm import SVC  # For support vector classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier  # For gradient boosting classification\n",
    "from sklearn.neural_network import MLPClassifier  # For multi-layer perceptron classification\n",
    "import random  # For random number generation and sampling\n",
    "from scipy import stats  # For statistical functions\n",
    "import matplotlib.pyplot as plt  # For plotting (duplicate import, same as above)\n",
    "import seaborn as sns # For visualization\n",
    "import shap # For explainability\n",
    "import gc  # For garbage collection\n",
    "\n",
    "base_dir = r\"E:\\Data Sorted\\Music\"\n",
    "\n",
    "directories = os.listdir(base_dir)\n",
    "directories = sorted(directories, key=lambda x: int(x))\n",
    "\n",
    "# Enumerate and print the sorted directories\n",
    "for count, directory in enumerate(directories, start=0):\n",
    "        \n",
    "    # Find all file paths matching the specified pattern\n",
    "    music_path = glob(os.path.join(r\"E:\\Data Sorted\\Music\", directory, \"*\"))\n",
    "\n",
    "    path = music_path\n",
    "    save_path = count + 1\n",
    "\n",
    "    # Filter file paths to separate positive and negative groups\n",
    "    pos_file_path = sorted([i for i in path if 'pos' in os.path.basename(i)])\n",
    "    neg_file_path = sorted([i for i in path if 'neg' in os.path.basename(i)])\n",
    "\n",
    "    # Print the number of files in each category\n",
    "    print(\"Number of pos files:\", len(pos_file_path))\n",
    "    print(\"Number of neg files:\", len(neg_file_path))\n",
    "\n",
    "    '''# Store initial sample sizes before undersampling\n",
    "    pre_undersampling = [len(pos_file_path), len(neg_file_path)]\n",
    "\n",
    "    # Determine the smaller class size between positive and negative groups\n",
    "    min_class_size = min(len(pos_file_path), len(neg_file_path))\n",
    "\n",
    "    # Undersample the larger class to balance the dataset\n",
    "    random.seed(42)  # For reproducibility\n",
    "    if len(pos_file_path) > min_class_size:\n",
    "        pos_file_path = random.sample(pos_file_path, min_class_size)\n",
    "    elif len(neg_file_path) > min_class_size:\n",
    "        neg_file_path = random.sample(neg_file_path, min_class_size)\n",
    "\n",
    "    # Print the new number of files in each category after undersampling\n",
    "    print(\"Number of pos files after undersampling:\", len(pos_file_path))\n",
    "    print(\"Number of neg files after undersampling:\", len(neg_file_path))\n",
    "\n",
    "    # Plotting the sample size pre and post-undersampling\n",
    "    post_undersampling = [len(pos_file_path), len(neg_file_path)]\n",
    "    categories = ['Positive', 'Negative']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(categories, pre_undersampling, color='blue', alpha=0.5, label='Pre Undersampling')\n",
    "    plt.bar(categories, post_undersampling, color='red', alpha=0.5, label='Post Undersampling')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Sample Size')\n",
    "    plt.title('Sample Size Pre and Post Undersampling')\n",
    "    plt.legend()\n",
    "    for i, value in enumerate(pre_undersampling):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom', fontsize=10)\n",
    "    for i, value in enumerate(post_undersampling):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom', fontsize=10)\n",
    "    plt.show()'''\n",
    "\n",
    "    # Store initial sample sizes before oversampling\n",
    "    pre_oversampling = [len(pos_file_path), len(neg_file_path)]\n",
    "\n",
    "    # Determine the larger class size between positive and negative groups\n",
    "    max_class_size = max(len(pos_file_path), len(neg_file_path))\n",
    "\n",
    "    # Oversample the smaller class to balance the dataset\n",
    "    random.seed(42)  # For reproducibility\n",
    "    if len(pos_file_path) < max_class_size:\n",
    "        pos_file_path = pos_file_path * (max_class_size // len(pos_file_path)) + random.sample(pos_file_path, max_class_size % len(pos_file_path))\n",
    "    elif len(neg_file_path) < max_class_size:\n",
    "        neg_file_path = neg_file_path * (max_class_size // len(neg_file_path)) + random.sample(neg_file_path, max_class_size % len(neg_file_path))\n",
    "\n",
    "    # Print the new number of files in each category after oversampling\n",
    "    print(\"Number of pos files after oversampling:\", len(pos_file_path))\n",
    "    print(\"Number of neg files after oversampling:\", len(neg_file_path))\n",
    "\n",
    "    # Plotting the sample size pre and post-oversampling\n",
    "    post_oversampling = [len(pos_file_path), len(neg_file_path)]\n",
    "    categories = ['Positive', 'Negative']\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(categories, pre_oversampling, color='blue', alpha=0.5, label='Pre Oversampling')\n",
    "    plt.bar(categories, post_oversampling, color='red', alpha=0.5, label='Post Oversampling')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Sample Size')\n",
    "    plt.title('Sample Size Pre and Post Oversampling')\n",
    "    plt.legend()\n",
    "    for i, value in enumerate(pre_oversampling):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom', fontsize=10)\n",
    "    for i, value in enumerate(post_oversampling):\n",
    "        plt.text(i, value, str(value), ha='center', va='bottom', fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "     \n",
    "    # Reference Electrodes for Music Tasks\n",
    "    selected_electrodes_music = [\n",
    "        'Fp1', 'F7', 'Fp2', 'F8', 'F4', 'AF3', 'AF4', 'AF7', 'AF8', \n",
    "        'F3', 'F5', 'F6', 'FC1', 'FC2', 'FC5', 'FC6', 'FT7', 'FT8', \n",
    "        'C3', 'C4', 'C5', 'C6', 'T7', 'T8', 'TP7', 'TP8', 'CP1', 'CP2', \n",
    "        'CP5', 'CP6', 'P3', 'P4', 'P5', 'P6', 'P7', 'P8', 'PO3', 'PO4', \n",
    "        'PO7', 'PO8', 'O1', 'O2'\n",
    "    ]\n",
    "\n",
    "    # Reading data from EEG files\n",
    "    def read_data(file_path, electrodes):\n",
    "        # Load EEG data from file\n",
    "        data = mne.io.read_raw_eeglab(file_path, preload=True);\n",
    "        # Set EEG reference channels\n",
    "        data.set_eeg_reference(ref_channels=[\"Fz\"])\n",
    "        # Apply bandpass filter to EEG data\n",
    "        data.filter(l_freq=4, h_freq=45)\n",
    "        # Select specific EEG channels\n",
    "        data.pick_channels(electrodes)\n",
    "        # Create fixed-length epochs from EEG data\n",
    "        epochs = mne.make_fixed_length_epochs(data, duration=15, overlap=1)\n",
    "        # Extract data from epochs\n",
    "        array = epochs.get_data()\n",
    "        return array\n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "    # Suppress output capture for this cell\n",
    "    # Read EEG data from the first positive file path and output set of electrodes on a dummy head\n",
    "    \n",
    "    #%%capture\n",
    "    head_pre = mne.io.read_raw_eeglab(pos_file_path[0], preload=True);\n",
    "    head_pre.pick_channels(selected_electrodes_music);\n",
    "\n",
    "     \n",
    "    # Create a standard montage for the \"easycap-M1\" electrode configuration\n",
    "    easycap_montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "    # Set the montage to the EEG data\n",
    "    head_pre.set_montage(easycap_montage)\n",
    "    # Plot the sensors with their names shown\n",
    "    fig = head_pre.plot_sensors(show_names=True)\n",
    "\n",
    "     \n",
    "\n",
    "    # Read EEG data for positive class from all positive file paths\n",
    "    # Read EEG data for negative class from all negative file paths\n",
    "\n",
    "    #%%capture\n",
    "    pos_epochs_array = [read_data(i, selected_electrodes_music) for i in pos_file_path];\n",
    "    neg_epochs_array = [read_data(i, selected_electrodes_music) for i in neg_file_path];\n",
    "\n",
    "     \n",
    "    # Create labels for positive epochs (0 for positive class)\n",
    "    pos_epoch_labels = [len(i) * [0] for i in pos_epochs_array]\n",
    "    # Create labels for negative epochs (1 for negative class)\n",
    "    neg_epoch_labels = [len(i) * [1] for i in neg_epochs_array]\n",
    "    # Check the length of positive and negative epoch labels\n",
    "    len(pos_epoch_labels), len(neg_epoch_labels)\n",
    "\n",
    "\n",
    "     \n",
    "    # Combine positive and negative epoch arrays into a single list\n",
    "    data_list = pos_epochs_array + neg_epochs_array\n",
    "    # Combine positive and negative epoch labels into a single list\n",
    "    label_list = pos_epoch_labels + neg_epoch_labels\n",
    "\n",
    "     \n",
    "    # Create a list of groups based on the number of epochs in each class\n",
    "    group_list = [[i] * len(j) for i, j in enumerate(data_list)]\n",
    "\n",
    "\n",
    "     \n",
    "    # Stack the data list vertically to create a 2D array\n",
    "    data_array = np.vstack(data_list)\n",
    "\n",
    "    # Stack the label list horizontally to create a 1D array\n",
    "    label_array = np.hstack(label_list)\n",
    "\n",
    "    # Stack the group list horizontally to create a 1D array\n",
    "    group_array = np.hstack(group_list)\n",
    "\n",
    "    del data_list, pos_epochs_array, neg_epochs_array  # Release memory\n",
    "    gc.collect()\n",
    "    def mean(x):\n",
    "        # Calculate the mean along the last axis of the input array.\n",
    "        return np.mean(x, axis=-1)\n",
    "\n",
    "    def std(x):\n",
    "        # Calculate the standard deviation along the last axis of the input array.\n",
    "        return np.std(x, axis=-1)\n",
    "\n",
    "    def ptp(x):\n",
    "        # Calculate the peak-to-peak (range) along the last axis of the input array.\n",
    "        return np.ptp(x, axis=-1)\n",
    "\n",
    "    def var(x):\n",
    "        # Calculate the variance along the last axis of the input array.\n",
    "        return np.var(x, axis=-1)\n",
    "\n",
    "    def minim(x):\n",
    "        # Find the minimum value along the last axis of the input array.\n",
    "        return np.min(x, axis=-1)\n",
    "\n",
    "    def maxim(x):\n",
    "        # Find the maximum value along the last axis of the input array.\n",
    "        return np.max(x, axis=-1)\n",
    "\n",
    "    def argminim(x):\n",
    "        # Find the index of the minimum value along the last axis of the input array.\n",
    "        return np.argmin(x, axis=-1)\n",
    "\n",
    "    def argmaxim(x):\n",
    "        # Find the index of the maximum value along the last axis of the input array.\n",
    "        return np.argmax(x, axis=-1)\n",
    "\n",
    "    def rms(x):\n",
    "        # Calculate the root mean square along the last axis of the input array.\n",
    "        return np.sqrt(np.mean(x**2, axis=-1))\n",
    "\n",
    "    def abs_diff_signal(x):\n",
    "        # Calculate the sum of absolute differences between consecutive elements along the last axis of the input array.\n",
    "        return np.sum(np.abs(np.diff(x, axis=-1)), axis=-1)\n",
    "\n",
    "    def skewness(x):\n",
    "        # Calculate the skewness along the last axis of the input array.\n",
    "        return stats.skew(x, axis=-1)\n",
    "\n",
    "    def kurtosis(x):\n",
    "        # Calculate the kurtosis along the last axis of the input array.\n",
    "        return stats.kurtosis(x, axis=-1)\n",
    "\n",
    "    def concatenate_features(x):\n",
    "        \"\"\"\n",
    "        Concatenate statistical features computed from the input array along the last axis.\n",
    "\n",
    "        Parameters:\n",
    "            x (ndarray): Input array containing signals.\n",
    "\n",
    "        Returns:\n",
    "            ndarray: Concatenated array of statistical features.\n",
    "        \"\"\"\n",
    "        return np.concatenate((mean(x), std(x), ptp(x), var(x), minim(x), maxim(x), argminim(x), argmaxim(x), rms(x), abs_diff_signal(x), skewness(x), kurtosis(x)), axis=-1)\n",
    "\n",
    "\n",
    "     \n",
    "    features = []\n",
    "\n",
    "    # Compute statistical features for each epoch in the data array and append to the features list\n",
    "    for d in data_array:\n",
    "        features.append(concatenate_features(d))\n",
    "    \n",
    "    del data_array  # Release memory\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert the features list to a numpy array\n",
    "    features_array = np.array(features)\n",
    "\n",
    "     \n",
    "    gkf = GroupKFold(5)\n",
    "\n",
    "    # Random Forest Classifier: Ensemble learning method for classification that operates by constructing a multitude of decision trees.\n",
    "    # The pipeline includes standard scaling of features, and grid search for tuning hyperparameters (number of estimators and maximum depth).\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_rf = RandomForestClassifier()\n",
    "    pipe_rf = Pipeline([('scaler', StandardScaler()), ('clf', clf_rf)])\n",
    "    param_grid_rf = {'clf__n_estimators': [50, 100, 200],  \n",
    "                    'clf__max_depth': [None, 10, 20]}\n",
    "    gscv_rf = GridSearchCV(pipe_rf, param_grid=param_grid_rf, cv=gkf, n_jobs=-1)\n",
    "    gscv_rf.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_rf = gscv_rf.best_score_\n",
    "\n",
    "    # Logistic Regression Classifier: Linear model for binary classification with adjustable regularization.\n",
    "    # The pipeline includes standard scaling of features, and grid search for tuning regularization strength (C).\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_lr = LogisticRegression(max_iter=1000)\n",
    "    pipe_lr = Pipeline([('scaler', StandardScaler()), ('clf', clf_lr)])\n",
    "    param_grid_lr = {'clf__C': [0.1, 0.5, 0.7, 1, 3, 5, 7]}\n",
    "    gscv_lr = GridSearchCV(pipe_lr, param_grid=param_grid_lr, cv=gkf)\n",
    "    gscv_lr.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_lr = gscv_lr.best_score_\n",
    "\n",
    "    # Naive Bayes (GaussianNB) Classifier: Simple probabilistic classifier based on Bayes' theorem with strong (naive) independence assumptions.\n",
    "    # The pipeline includes standard scaling of features. No hyperparameters are tuned.\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_nb = GaussianNB()\n",
    "    pipe_nb = Pipeline([('scaler', StandardScaler()), ('clf', clf_nb)])\n",
    "    gscv_nb = GridSearchCV(pipe_nb, param_grid={}, cv=gkf)\n",
    "    gscv_nb.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_nb = gscv_nb.best_score_\n",
    "\n",
    "    # Support Vector Machines (SVM) Classifier: Effective for high-dimensional spaces, especially when the number of features exceeds the number of samples.\n",
    "    # The pipeline includes standard scaling of features, and grid search for tuning regularization strength (C) and kernel type.\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_svm = SVC()\n",
    "    pipe_svm = Pipeline([('scaler', StandardScaler()), ('clf', clf_svm)])\n",
    "    param_grid_svm = {'clf__C': [0.1, 1, 10], 'clf__kernel': ['linear', 'rbf']}\n",
    "    gscv_svm = GridSearchCV(pipe_svm, param_grid=param_grid_svm, cv=gkf, n_jobs=-1)\n",
    "    gscv_svm.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_svm = gscv_svm.best_score_\n",
    "\n",
    "    # Gradient Boosting Machines (GBM) Classifier: Builds an ensemble of weak learners (typically decision trees), sequentially improving performance.\n",
    "    # The pipeline includes standard scaling of features, and grid search for tuning learning rate, maximum depth, and number of estimators.\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_gbm = GradientBoostingClassifier(random_state=42)\n",
    "    pipe_gbm = Pipeline([('scaler', StandardScaler()), ('clf', clf_gbm)])\n",
    "    param_grid_gbm = {'clf__learning_rate': [0.01, 0.1], 'clf__max_depth': [3, 5], 'clf__n_estimators': [50, 100]}\n",
    "    gscv_gbm = GridSearchCV(pipe_gbm, param_grid=param_grid_gbm, cv=gkf, n_jobs=-1)\n",
    "    gscv_gbm.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_gbm = gscv_gbm.best_score_\n",
    "\n",
    "    # Neural Networks Classifier: Multilayer perceptron with customizable architecture and activation functions.\n",
    "    # The pipeline includes standard scaling of features, and grid search for tuning hidden layer sizes and activation functions.\n",
    "    # Cross-validation is performed using GroupKFold to ensure group-wise data splitting.\n",
    "    clf_nn = MLPClassifier(random_state=42)\n",
    "    pipe_nn = Pipeline([('scaler', StandardScaler()), ('clf', clf_nn)])\n",
    "    param_grid_nn = {'clf__hidden_layer_sizes': [(50,), (100,), (50, 50)], 'clf__activation': ['relu', 'tanh']}\n",
    "    gscv_nn = GridSearchCV(pipe_nn, param_grid=param_grid_nn, cv=gkf, n_jobs=-1)\n",
    "    gscv_nn.fit(features_array, label_array, groups=group_array)\n",
    "    best_score_nn = gscv_nn.best_score_\n",
    "\n",
    "     \n",
    "    print(\"Best Score for Logistic Regression:\", best_score_lr)\n",
    "    print(\"Best Score for Random Forest:\", best_score_rf)\n",
    "    print(\"Best Score for Naive Bayes:\", best_score_nb)\n",
    "    print(\"Best Score for Support Vector Machines:\", best_score_svm)\n",
    "    print(\"Best Score for Gradient Boosting Machines:\", best_score_gbm)\n",
    "    print(\"Best Score for Neural Networks:\", best_score_nn)\n",
    "\n",
    "    # Define the model names and their corresponding best scores\n",
    "    model_names = ['Logistic Regression', 'Random Forest', 'Naive Bayes', \n",
    "                'Support Vector Machines', 'Gradient Boosting Machines', 'Neural Networks']\n",
    "    best_scores = [best_score_lr, best_score_rf, best_score_nb, \n",
    "                best_score_svm, best_score_gbm, best_score_nn]\n",
    "\n",
    "    # Sort the model names and best scores\n",
    "    sorted_data = sorted(zip(model_names, best_scores), key=lambda x: x[1])\n",
    "\n",
    "    # Unzip the sorted data\n",
    "    sorted_model_names, sorted_best_scores = zip(*sorted_data)\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(sorted_model_names, sorted_best_scores, color='skyblue')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Best Model Scores')\n",
    "    plt.xlim(0, 1)  # Setting limit from 0 to 1 for accuracy\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
    "\n",
    "    # Annotate each bar with its specific score\n",
    "    for bar, score in zip(bars, sorted_best_scores):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{score:.3f}', \n",
    "                va='center', ha='left', fontsize=10, color='black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "     \n",
    "    # List of statistical features\n",
    "    statistical_features = ['mean', 'std', 'ptp', 'var', 'min', 'max', 'argmin', 'argmax', 'rms', 'abs_diff_signal', 'skewness', 'kurtosis']\n",
    "\n",
    "    # Generate detailed feature names\n",
    "    detailed_feature_names = []\n",
    "    for electrode in selected_electrodes_music:\n",
    "        for feature in statistical_features:\n",
    "            detailed_feature_names.append(f\"{electrode}_{feature}\")\n",
    "\n",
    "    # Check the length of detailed_feature_names\n",
    "    print(\"Length of detailed_feature_names:\", len(detailed_feature_names))\n",
    "\n",
    "     \n",
    "    # Define the feature names based on electrodes and feature functions\n",
    "    feature_functions = ['mean', 'std', 'ptp', 'var', 'min', 'max', 'argmin', 'argmax', 'rms', 'abs_diff_signal', 'skewness', 'kurtosis']\n",
    "    electrode_feature_names = []\n",
    "    for electrode in selected_electrodes_music:\n",
    "        for feature in feature_functions:\n",
    "            electrode_feature_names.append(f\"{electrode}_{feature}\")\n",
    "\n",
    "    # Ensure the number of features matches\n",
    "    if len(electrode_feature_names) != features_array.shape[1]:\n",
    "        raise ValueError(\"The number of electrode feature names does not match the number of features in the data\")\n",
    "\n",
    "    # Use TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(gscv_rf.best_estimator_.named_steps['clf'])\n",
    "    shap_values = explainer.shap_values(features_array)\n",
    "\n",
    "    # Convert SHAP values to a DataFrame for easier manipulation\n",
    "    shap_values_df = pd.DataFrame(shap_values[1], columns=electrode_feature_names)\n",
    "\n",
    "    # Aggregate SHAP values by summing all features for each electrode\n",
    "    electrode_shap_values = {}\n",
    "    for electrode in selected_electrodes_music:\n",
    "        electrode_shap_values[electrode] = shap_values_df.filter(like=electrode).sum(axis=1).mean()\n",
    "\n",
    "    # Convert to DataFrame for visualization\n",
    "    electrode_shap_values_df = pd.DataFrame(electrode_shap_values.items(), columns=['Electrode', 'SHAP Value'])\n",
    "\n",
    "    # Sort values for better visualization\n",
    "    electrode_shap_values_df = electrode_shap_values_df.sort_values(by='SHAP Value', ascending=False)\n",
    "\n",
    "     \n",
    "    # Sort and select the top N channels\n",
    "    top_n = 20\n",
    "    top_electrode_shap_values_df = electrode_shap_values_df.head(top_n)\n",
    "\n",
    "    # Plot with Seaborn for better aesthetics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        x='SHAP Value', \n",
    "        y='Electrode', \n",
    "        data=top_electrode_shap_values_df,\n",
    "        palette='viridis'\n",
    "    )\n",
    "    plt.xlabel('Mean SHAP Value')\n",
    "    plt.title(f'Top {top_n} Electrode Importance (Aggregated SHAP Values)')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "    plt.show()\n",
    "\n",
    "    # Get the list of top 20 electrodes\n",
    "    top_20_electrodes = top_electrode_shap_values_df['Electrode'].tolist()\n",
    "    print(\"Top 20 Electrodes are: \", top_20_electrodes)\n",
    "\n",
    "     \n",
    "    # Define the feature names based on electrodes and feature functions\n",
    "    feature_functions = ['mean', 'std', 'ptp', 'var', 'min', 'max', 'argmin', 'argmax', 'rms', 'abs_diff_signal', 'skewness', 'kurtosis']\n",
    "    electrode_feature_names = []\n",
    "\n",
    "    # Iterate through each electrode and feature function to generate detailed feature names\n",
    "    for electrode in selected_electrodes_music:\n",
    "        for feature in feature_functions:\n",
    "            electrode_feature_names.append(f\"{electrode}_{feature}\")\n",
    "\n",
    "    # Ensure the number of features matches the length of the electrode feature names\n",
    "    if len(electrode_feature_names) != features_array.shape[1]:\n",
    "        raise ValueError(\"The number of electrode feature names does not match the number of features in the data\")\n",
    "\n",
    "    # Use TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(gscv_rf.best_estimator_.named_steps['clf'])\n",
    "    shap_values = explainer.shap_values(features_array)\n",
    "\n",
    "    # Extract SHAP values for class 1 (assuming binary classification)\n",
    "    shap_values_class1 = shap_values[1]\n",
    "\n",
    "    # Convert SHAP values to a DataFrame for easier manipulation\n",
    "    shap_values_df = pd.DataFrame(shap_values_class1, columns=electrode_feature_names)\n",
    "\n",
    "    # Aggregate SHAP values by summing all features for each electrode\n",
    "    electrode_shap_values = {}\n",
    "    for electrode in selected_electrodes_music:\n",
    "        electrode_shap_values[electrode] = shap_values_df.filter(like=electrode).sum(axis=1).mean()\n",
    "\n",
    "    # Convert to DataFrame for visualization\n",
    "    electrode_shap_values_df = pd.DataFrame(electrode_shap_values.items(), columns=['Electrode', 'SHAP Value'])\n",
    "\n",
    "    # Sort values for better visualization\n",
    "    electrode_shap_values_df = electrode_shap_values_df.sort_values(by='SHAP Value', ascending=False)\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(electrode_shap_values_df['Electrode'], electrode_shap_values_df['SHAP Value'])\n",
    "    plt.xlabel('Mean SHAP Value')\n",
    "    plt.title('Electrode Importance (Aggregated SHAP Values)')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    del features_array, label_array, group_array  # Release memory\n",
    "    gc.collect()\n",
    "\n",
    "     \n",
    "    \n",
    "    # Capture cell output to suppress any displayed output\n",
    "\n",
    "    #%%capture\n",
    "    top_20_electrodes_df = electrode_shap_values_df.head(top_n)\n",
    "\n",
    "    top_20_pos_epochs_array = [read_data(i, top_20_electrodes) for i in pos_file_path];\n",
    "    top_20_neg_epochs_array = [read_data(i, top_20_electrodes) for i in neg_file_path];\n",
    "\n",
    "    # Read EEG data from a positive file for the top 20 electrodes\n",
    "    head_post = mne.io.read_raw_eeglab(pos_file_path[0], preload=True);\n",
    "    head_post.pick_channels(top_20_electrodes)\n",
    "\n",
    "     \n",
    "    # Define the montage using the \"easycap-M1\" standard\n",
    "    easycap_montage = mne.channels.make_standard_montage(\"easycap-M1\")\n",
    "\n",
    "    # Set the montage for the EEG data\n",
    "    head_post.set_montage(easycap_montage)\n",
    "\n",
    "    # Plot the sensors with names\n",
    "    fig = head_post.plot_sensors(show_names=True)\n",
    "\n",
    "     \n",
    "    # Define labels for positive and negative epochs based on the top 20 electrodes\n",
    "    top_20_pos_epoch_labels = [len(i) * [0] for i in top_20_pos_epochs_array]\n",
    "    top_20_neg_epoch_labels = [len(i) * [1] for i in top_20_neg_epochs_array]\n",
    "\n",
    "    # Combine the labels for positive and negative epochs\n",
    "    top_20_label_list = top_20_pos_epoch_labels + top_20_neg_epoch_labels\n",
    "\n",
    "    # Create a list to group data based on epochs\n",
    "    top_20_group_list = [[i] * len(j) for i, j in enumerate(top_20_pos_epochs_array + top_20_neg_epochs_array)]\n",
    "\n",
    "    # Convert data and labels into arrays\n",
    "    top_20_data_array = np.vstack(top_20_pos_epochs_array + top_20_neg_epochs_array)\n",
    "    top_20_label_array = np.hstack(top_20_label_list)\n",
    "    top_20_group_array = np.hstack(top_20_group_list)\n",
    "\n",
    "    # Release memory used by epoch arrays\n",
    "    del top_20_pos_epochs_array, top_20_neg_epochs_array, top_20_label_list, top_20_group_list\n",
    "    gc.collect()\n",
    "\n",
    "    # Extract statistical features from the data\n",
    "    top_20_features = []\n",
    "    for d in top_20_data_array:\n",
    "        top_20_features.append(concatenate_features(d))\n",
    "\n",
    "    # Convert the features into an array\n",
    "    top_20_features_array = np.array(top_20_features)\n",
    "\n",
    "    # Define a GroupKFold cross-validation iterator\n",
    "    top_20_gkf = GroupKFold(5)\n",
    "\n",
    "    # Release memory used by data array\n",
    "    del top_20_data_array, top_20_features\n",
    "    gc.collect()\n",
    "\n",
    "    # Define the Random Forest classifier\n",
    "    top_20_clf_rf = RandomForestClassifier()\n",
    "    top_20_pipe_rf = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_rf)])\n",
    "    top_20_param_grid_rf = {'clf__n_estimators': [50, 100, 200],  \n",
    "                            'clf__max_depth': [None, 10, 20]}\n",
    "    top_20_gscv_rf = GridSearchCV(top_20_pipe_rf, param_grid=top_20_param_grid_rf, cv=top_20_gkf, n_jobs=-1)\n",
    "    top_20_gscv_rf.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_rf = top_20_gscv_rf.best_score_\n",
    "\n",
    "    # Define the Logistic Regression classifier\n",
    "    top_20_clf_lr = LogisticRegression(max_iter=1000)  # Example max_iter value\n",
    "    top_20_pipe_lr = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_lr)])\n",
    "    top_20_param_grid_lr = {'clf__C': [0.1, 0.5, 0.7, 1, 3, 5, 7]}  # Example values for regularization strength\n",
    "    top_20_gscv_lr = GridSearchCV(top_20_pipe_lr, param_grid=top_20_param_grid_lr, cv=top_20_gkf)\n",
    "    top_20_gscv_lr.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_lr = top_20_gscv_lr.best_score_\n",
    "\n",
    "    # Naive Bayes (GaussianNB)\n",
    "    top_20_clf_nb = GaussianNB()\n",
    "    top_20_pipe_nb = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_nb)])\n",
    "    top_20_gscv_nb = GridSearchCV(top_20_pipe_nb, param_grid={}, cv=top_20_gkf)\n",
    "    top_20_gscv_nb.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_nb = top_20_gscv_nb.best_score_\n",
    "\n",
    "    # Support Vector Machines (SVM)\n",
    "    top_20_clf_svm = SVC()\n",
    "    top_20_pipe_svm = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_svm)])\n",
    "    top_20_param_grid_svm = {'clf__C': [0.1, 1, 10], 'clf__kernel': ['linear', 'rbf']}\n",
    "    top_20_gscv_svm = GridSearchCV(top_20_pipe_svm, param_grid=top_20_param_grid_svm, cv=top_20_gkf, n_jobs=-1)\n",
    "    top_20_gscv_svm.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_svm = top_20_gscv_svm.best_score_\n",
    "\n",
    "    # Gradient Boosting Machines (GBM)\n",
    "    top_20_clf_gbm = GradientBoostingClassifier(random_state=42)\n",
    "    top_20_pipe_gbm = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_gbm)])\n",
    "    top_20_param_grid_gbm = {'clf__learning_rate': [0.01, 0.1], 'clf__max_depth': [3, 5], 'clf__n_estimators': [50, 100]}\n",
    "    top_20_gscv_gbm = GridSearchCV(top_20_pipe_gbm, param_grid=top_20_param_grid_gbm, cv=top_20_gkf, n_jobs=-1)\n",
    "    top_20_gscv_gbm.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_gbm = top_20_gscv_gbm.best_score_\n",
    "\n",
    "    # Neural Networks\n",
    "    top_20_clf_nn = MLPClassifier(random_state=42)\n",
    "    top_20_pipe_nn = Pipeline([('scaler', StandardScaler()), ('clf', top_20_clf_nn)])\n",
    "    top_20_param_grid_nn = {'clf__hidden_layer_sizes': [(50,), (100,), (50, 50)], 'clf__activation': ['relu', 'tanh']}\n",
    "    top_20_gscv_nn = GridSearchCV(top_20_pipe_nn, param_grid=top_20_param_grid_nn, cv=top_20_gkf, n_jobs=-1)\n",
    "    top_20_gscv_nn.fit(top_20_features_array, top_20_label_array, groups=top_20_group_array)\n",
    "    top_20_best_score_nn = top_20_gscv_nn.best_score_\n",
    "\n",
    "     \n",
    "    # Print the best scores for each model\n",
    "    print(\"Best Score for Logistic Regression:\", top_20_best_score_lr)\n",
    "    print(\"Best Score for Random Forest:\", top_20_best_score_rf)\n",
    "    print(\"Best Score for Naive Bayes:\", top_20_best_score_nb)\n",
    "    print(\"Best Score for Support Vector Machines:\", top_20_best_score_svm)\n",
    "    print(\"Best Score for Gradient Boosting Machines:\", top_20_best_score_gbm)\n",
    "    print(\"Best Score for Neural Networks:\", top_20_best_score_nn)\n",
    "\n",
    "    # Define the model names and their corresponding best scores\n",
    "    top_20_model_names = ['Logistic Regression', 'Random Forest', 'Naive Bayes', \n",
    "                        'Support Vector Machines', 'Gradient Boosting Machines', 'Neural Networks']\n",
    "    top_20_best_scores = [top_20_best_score_lr, top_20_best_score_rf, top_20_best_score_nb, \n",
    "                        top_20_best_score_svm, top_20_best_score_gbm, top_20_best_score_nn]\n",
    "\n",
    "    # Sort the model names and best scores\n",
    "    top_20_sorted_data = sorted(zip(top_20_model_names, top_20_best_scores), key=lambda x: x[1])\n",
    "\n",
    "    # Unzip the sorted data\n",
    "    top_20_sorted_model_names, top_20_sorted_best_scores = zip(*top_20_sorted_data)\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_20_bars = plt.barh(top_20_sorted_model_names, top_20_sorted_best_scores, color='green')\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.title('Best Model Scores based on the top scoring 20 electrodes')\n",
    "    plt.xlim(0, 1)  # Setting limit from 0 to 1 for accuracy\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
    "\n",
    "    # Annotate each bar with its specific score\n",
    "    for bar, score in zip(top_20_bars, top_20_sorted_best_scores):\n",
    "        plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f'{score:.3f}', \n",
    "                va='center', ha='left', fontsize=10, color='black')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Release memory used by features array\n",
    "    del top_20_features_array, top_20_label_array, top_20_group_array\n",
    "    gc.collect()\n",
    "     \n",
    "    def save_results_to_file(results_filename, plot_directory):\n",
    "            # Create directory if it doesn't exist\n",
    "        if not os.path.exists(plot_directory):\n",
    "            os.makedirs(plot_directory)\n",
    "\n",
    "        # Save results to a text file\n",
    "        with open(results_filename, 'w') as file:\n",
    "            file.write(\"Best Scores for Each Model:\\n\")\n",
    "            file.write(f\"Logistic Regression: {best_score_lr}\\n\")\n",
    "            file.write(f\"Random Forest: {best_score_rf}\\n\")\n",
    "            file.write(f\"Naive Bayes: {best_score_nb}\\n\")\n",
    "            file.write(f\"Support Vector Machines: {best_score_svm}\\n\")\n",
    "            file.write(f\"Gradient Boosting Machines: {best_score_gbm}\\n\")\n",
    "            file.write(f\"Neural Networks: {best_score_nn}\\n\")\n",
    "            file.write(\"\\nElectrodes:\\n\")\n",
    "            file.write(', '.join(selected_electrodes_music))\n",
    "\n",
    "            file.write(\"\\nTop 20 Electrodes:\\n\")\n",
    "            file.write(', '.join(top_20_electrodes))\n",
    "\n",
    "            file.write(\"\\n\\nBest Scores for Each Model (Top 20 Electrodes):\\n\")\n",
    "            file.write(f\"Logistic Regression: {top_20_best_score_lr}\\n\")\n",
    "            file.write(f\"Random Forest: {top_20_best_score_rf}\\n\")\n",
    "            file.write(f\"Naive Bayes: {top_20_best_score_nb}\\n\")\n",
    "            file.write(f\"Support Vector Machines: {top_20_best_score_svm}\\n\")\n",
    "            file.write(f\"Gradient Boosting Machines: {top_20_best_score_gbm}\\n\")\n",
    "            file.write(f\"Neural Networks: {top_20_best_score_nn}\\n\")\n",
    "\n",
    "        # Save plots and graphs to the directory\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.barh(sorted_model_names, sorted_best_scores, color='skyblue')\n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.title('Best Model Scores')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.savefig(os.path.join(plot_directory, 'best_model_scores.png'))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='SHAP Value', y='Electrode', data=top_electrode_shap_values_df, palette='viridis')\n",
    "        plt.xlabel('Mean SHAP Value')\n",
    "        plt.title(f'Top {top_n} Electrode Importance (Aggregated SHAP Values)')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.savefig(os.path.join(plot_directory, 'top_electrode_importance.png'))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_20_bars = plt.barh(top_20_sorted_model_names, top_20_sorted_best_scores, color='green')\n",
    "        plt.xlabel('Accuracy')\n",
    "        plt.title('Best Model Scores based on the top scoring 20 electrodes')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.savefig(os.path.join(plot_directory, 'best_model_scores_top_20_electrodes.png'))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        categories = ['Positive', 'Negative']\n",
    "        #plt.bar(categories, pre_undersampling, color='blue', alpha=0.5, label='Pre Undersampling')\n",
    "        #plt.bar(categories, post_undersampling, color='red', alpha=0.5, label='Post Undersampling')\n",
    "        plt.bar(categories, pre_oversampling , color='blue', alpha=0.5, label='Pre Undersampling')\n",
    "        plt.bar(categories, post_oversampling, color='red', alpha=0.5, label='Post Undersampling')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Sample Size')\n",
    "        plt.title('Sample Size Pre and Post Undersampling')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(plot_directory, 'sample_size_pre_post_undersampling.png'))\n",
    "        plt.close()\n",
    "\n",
    "        fig = head_pre.plot_sensors(show_names=True)\n",
    "        fig.savefig(os.path.join(plot_directory, 'sensors_with_names_before.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig = head_post.plot_sensors(show_names=True)\n",
    "        fig.savefig(os.path.join(plot_directory, 'sensors_with_names_top_20.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Plot electrode importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(electrode_shap_values_df['Electrode'], electrode_shap_values_df['SHAP Value'])\n",
    "        plt.xlabel('Mean SHAP Value')\n",
    "        plt.title('Electrode Importance (Aggregated SHAP Values)')\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis for better visualization=\n",
    "        plt.savefig(os.path.join(plot_directory, 'electrode_importance.png'))\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "    # Call the function to save results\n",
    "    results = \"results {}.txt\".format(str(save_path))\n",
    "    plots = \"plots {}/\".format(str(save_path))\n",
    "\n",
    "    save_results_to_file(results,plots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
